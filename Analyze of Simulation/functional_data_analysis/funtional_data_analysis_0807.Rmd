---
title: "first_learning"
author: "Chen"
date: "8/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(funFEM)
library(tidyverse)
library(readxl)
# install.packages("funFEM")
# library(openxlsx)
library(writexl)
setwd("D:/Files/College/Lab/EBG Sim/Analyze of Simulation/functional_data_analysis")
```

-   funFEM

    -   allows **to cluster time series or, more generally, functional data**.

    -   based on a discriminative functional mixture model which allows the clustering of the data in a unique and discriminative functional subspace.

    -   presents the advantage to be parsimonious and can therefore handle long time series.

# data combined

```{r}
setwd("data")
file_list <- list.files(pattern = "*).csv", recursive = T)
data <- do.call(rbind, lapply(file_list, read_csv))
write_csv(data, "combined2.csv")
```

## 2.2 Usage

    funFEM(fd, K=2:6, model = "AkjBk", crit = "bic", init = "hclust", Tinit = c(), maxit = 50, eps = 1e-06, disp = FALSE, lambda = 0, graph = FALSE)

```{r}
data <- read_csv("combined2.csv") %>% 
  filter(type != "times") 

row.names(data) <- data$type
data <- data %>% 
  select(-"type")

lndata <- log(data)

a <- data.matrix(data)#[,30:50]
lna <- data.matrix(lndata)#[,30:50]

# CanadianWeather_Temp <- CanadianWeather$dailyAv[,,"Temperature.C"]

c <- day.5[1:101]
names(c) <- 1:101
rownames(a) <- 1:101
rownames(lna) <- 1:101

# Clustering the well-known "Canadian temperature" data (Ramsay & Silverman)
basis <- create.bspline.basis(c(0, 101), nbasis=21, norder=4) # norder=4 : cubic spline
fdobj <- smooth.basis(c, a, 
                      basis, fdnames=list("Trial", "Entity", "Price"))$fd
lnfdobj <- smooth.basis(c, lna, 
                      basis, fdnames=list("Trial", "Entity", "Price"))$fd

res <- funFEM(fdobj, K=6)
lnres <- funFEM(lnfdobj, K=6)

# # Clustering the well-known "Canadian temperature" data (Ramsay & Silverman)
# basis <- create.bspline.basis(c(0, 365), nbasis=21, norder=4) # norder=4 : cubic spline
# fdobj <- smooth.basis(day.5, CanadianWeather_Temp, 
#                       basis, fdnames=list("Day", "Station", "Deg C"))$fd
# res <- funFEM(fdobj, K=4)


# Visualization of the partition and the group means
par(mfrow=c(1,2))

fdmeans <- fdobj

plot(fdobj, group=res$cls)


# plot(fdobj$coefs)
# plot(basis)
# plotfit.fd()

fdmeans$coefs <- t(res$prms$my)
plot(fdmeans, col=1:max(res$cls), lwd=2)
```

```{r}
# plot(fdmeans, col=1, lwd=2)



plotdata <- data %>% 
  mutate(trial = row_number()) %>% 
  pivot_longer(cols = !trial, names_to = "round", values_to = "price") %>% 
  pivot_wider(names_from = trial, values_from = price) %>% 
  cbind(res$cls) %>% 
  rename("cluster"="res$cls") %>% 
  pivot_longer(cols = !c(round, cluster), names_to = "trial", values_to = "price") %>% 
  mutate(trial = as.numeric(trial))

plotdata$p1 <- gsub(" VS [A-Za-z0-9_]+", "", plotdata$round)
plotdata$p2 <- gsub("^[A-Za-z]+ VS ", "", plotdata$round)
plotdata$p2 <- gsub("_[0-9]+$", "", plotdata$p2)
plotdata$player <- gsub("_[0-9]+$", "", plotdata$round)

plotdata %>% 
  # filter(cluster==1) %>% 
  # mutate(lnp = log(price)) %>% 
  ggplot(aes(x=trial, y=price, group=round, color=as.factor(cluster)))+
  geom_line( stat = "identity", position = "identity", alpha=0.5, size = 1) +
  scale_color_manual(values = c("chocolate1", "gold","indianred1",  "lightcyan3", "cyan", "mediumorchid1"))
  # scale_x_discrete(breaks = seq(0,100,10))
  

table <- plotdata %>% 
  select(round, player, cluster) %>% 
  unique() %>% 
  select(-round) %>% 
  count(cluster, player) %>% 
  pivot_wider(names_from = player, values_from = n) 

table[is.na(table)] <- 0

```


```{r}

res$aic
res$bic
res$icl

d6 <- data.frame(group = names(data), clust = res$cls) %>% 
  mutate(group = str_remove_all(group, "_[0-9]*")) %>% 
  group_by(group) %>% 
  count(clust) %>% 
  arrange(clust) %>% 
  pivot_wider(names_from = clust, values_from = n) 

d6[is.na(d6)] <- 0

d6 %>% write_xlsx("groupb15.xlsx")
```


```{r}
res$cls %>% summary()
res$P %>% round(2) %>%  View()

res$cls

names(data)
```

# teaching

```{r}
install.packages("fda")
library(fda)

tobs = seq(0,1,0.01)
nobs = length(tobs)
knots    = c(seq(0,1,0.1));
nknots   = length(knots);
norder   = 4;
nbasis   = length(knots) + norder - 2;
basis = create.bspline.basis(c(min(tobs),max(tobs)),nbasis,norder,knots);


# basis values at samplincurv points

basismat   = eval.basis(tobs, basis);
dim(basismat)
?quartz()
plot(tobs,basismat[,1],type = "l",col=1,lwd=3)
lines(tobs,basismat[,2],type = "l",col=2,lwd=3)

quartz()
matplot(tobs,basismat,type='l',lwd=2,lty=1, xlab='day',ylab='basis',cex.lab=1.5,cex.axis=1.5)
for (i in 1:nknots)
{
  abline(v=knots[i],type="l", lty=2, lwd=3)
}

# Comments: If x(0) = 1, set the coefficient to the first basis function = 1; 

# evaluate the first derivative of the basis functions
Dbasismat   = eval.basis(tobs, basis,1);
quartz()
matplot(tobs,Dbasismat,type='l',lwd=2,lty=1, xlab='day',ylab='basis',cex.lab=1.5,cex.axis=1.5)
for (i in 1:nknots)
{
  abline(v=knots[i],type="l", lty=2, lwd=3)
}

# true curve
ytru = (tobs-0.3)^2
plot(tobs,ytru,type = "l")

# put noise to the true curve and generate noisy data
nobs = length(tobs)
noise = 0.03*rnorm(nobs)
yobs = ytru + noise
points(tobs,yobs)

# estimate basis coefficient
Mmat = ginv(t(basismat)%*%basismat)%*%t(basismat)
chat = Mmat%*%yobs

# fitted curve
yhat = basismat%*%chat;
lines(tobs,yhat,type = "l",col="red")

# estimate the variance of noise
SSE = t(yhat-yobs)%*%(yhat-yobs)
sigma2 = SSE/(nobs-nbasis)
sigma2
sqrt(sigma2)
# estimate the variance of the fitted curve
Smat = basismat%*%Mmat
varYhat = diag(Smat%*%Smat*matrix(sigma2,nobs,nobs))

# 95% confidence interval

yhat025 = yhat-1.96*sqrt(varYhat)
yhat975 = yhat+1.96*sqrt(varYhat)

lines(tobs,yhat025,type="l", lty=2, lwd=3,col="blue")
lines(tobs,yhat975,type="l", lty=2, lwd=3,col="blue")

###########################
# Smoothing Splines
#########################


# Use quadrature to get integral - Composite Simpson's Rule

delta <- 0.02
quadpts <- seq(0,1,delta)
nquadpts <- length(quadpts)
quadwts <- as.vector(c(1,rep(c(4,2),(nquadpts-2)/2),4,1),mode="any")
quadwts <- c(1,rep(c(4,2),(nquadpts-1)/2))
quadwts[nquadpts] <- 1
quadwts <- quadwts*delta/3


# Second derivative of basis functions at quadrature points

Q2basismat   = eval.basis(quadpts, basis,2);

# estimates for basis coefficients
Rmat = t(Q2basismat)%*%(Q2basismat*(quadwts%*%t(rep(1,nbasis))))

dim(Rmat)
basismat2 = t(basismat)%*%basismat;
lambda = 0.05   # smoothing parameter
Bmat                      = basismat2 + lambda*Rmat;
chat = ginv(Bmat)%*%t(basismat)%*%yobs;

# fitted value
yhat = basismat%*%chat;
yhat2 = basismat%*%ginv(t(basismat)%*%basismat)%*%t(basismat)%*%yobs;

quartz()
plot(tobs,ytru,type = "l")
points(tobs,yobs)
lines(tobs,yhat,type = "l",col="red")
lines(tobs,yhat2,type = "l",col="blue")

# degrees of freedom
Mmat = ginv(Bmat)%*%t(basismat)
Smat = basismat%*%Mmat
df = sum(diag(Smat))
c(df,nbasis)
# estimate the variance of noise
SSE = t(yhat-yobs)%*%(yhat-yobs)
sigma2 = SSE/(nobs-df)
sigma2
sqrt(sigma2)
# estimate the variance of the fitted curve
varYhat = diag(Smat%*%Smat*matrix(sigma2,nobs,nobs))

# 95% confidence interval

yhat025 = yhat-1.96*sqrt(varYhat)
yhat975 = yhat+1.96*sqrt(varYhat)

lines(tobs,yhat025,type="l", lty=2, lwd=3,col="blue")
lines(tobs,yhat975,type="l", lty=2, lwd=3,col="blue")

```



